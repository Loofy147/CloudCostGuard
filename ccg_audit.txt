# CloudCostGuard: Production Engineering Audit

## Executive Summary

**Overall Assessment: 6.5/10** - Good MVP foundation, but needs significant hardening for production.

### Critical Issues (P0)
1. ❌ No request timeouts or circuit breakers
2. ❌ In-memory pricing cache lacks persistence/redundancy
3. ❌ No authentication on backend API
4. ❌ Missing structured logging and observability
5. ❌ No graceful shutdown handling
6. ❌ Database migrations not managed properly

### High Priority (P1)
7. ⚠️ Limited error context and recovery
8. ⚠️ No rate limiting on API endpoints
9. ⚠️ Missing health checks and readiness probes
10. ⚠️ Test coverage gaps (no integration tests visible)

---

## 1. Architecture Improvements

### Current Issues
- Monolithic backend main.go mixing concerns
- Global state (pricing cache) without proper synchronization patterns
- No service layer abstraction
- Direct database access throughout

### Recommended Architecture

```
backend/
├── cmd/
│   └── server/
│       └── main.go              # Thin entry point
├── internal/
│   ├── api/
│   │   ├── handlers/            # HTTP handlers
│   │   ├── middleware/          # Auth, logging, recovery
│   │   └── router.go            # Route configuration
│   ├── service/
│   │   ├── estimator/           # Business logic
│   │   ├── pricing/             # Pricing operations
│   │   └── interfaces.go        # Service interfaces
│   ├── repository/
│   │   ├── postgres/            # DB implementation
│   │   └── interfaces.go        # Repo interfaces
│   ├── cache/
│   │   └── pricing_cache.go     # Thread-safe cache
│   └── config/
│       └── config.go            # Config management
└── pkg/                         # Public packages
```

---

## 2. Code Quality Issues

### backend/main.go Problems

**Issue 1: Global Mutable State**
```go
var (
	pricingCache *pricing.PriceList
	cacheMutex   sync.RWMutex
)
```
**Fix:** Encapsulate in a service with proper lifecycle management

**Issue 2: No Graceful Shutdown**
```go
if err := http.ListenAndServe(":8080", nil); err != nil {
    log.Fatalf("Failed to start server: %v", err)
}
```
**Fix:** Implement graceful shutdown with signal handling

**Issue 3: No Request Timeouts**
```go
client := &http.Client{}
```
**Fix:** Configure timeouts for all HTTP clients

### backend/database/database.go Problems

**Issue: Credential Hardcoding**
```go
if connStr == "" {
    connStr = "user=postgres password=password dbname=pricing sslmode=disable"
}
```
**Fix:** Never include default credentials; fail fast if not configured

**Issue: No Connection Pooling Configuration**
```go
DB, err = sql.Open("postgres", connStr)
```
**Fix:** Set MaxOpenConns, MaxIdleConns, ConnMaxLifetime

### backend/estimator/estimator.go Problems

**Issue: Silent Error Handling**
```go
if err != nil {
    fmt.Printf("Warning: skipping unsupported resource %s: %v\n", rc.Address, err)
    continue
}
```
**Fix:** Use structured logging and error aggregation

---

## 3. Critical Production Fixes

### Fix 1: Proper Server Lifecycle

```go
// backend/cmd/server/main.go
package main

import (
    "context"
    "fmt"
    "net/http"
    "os"
    "os/signal"
    "syscall"
    "time"

    "cloudcostguard/internal/api"
    "cloudcostguard/internal/cache"
    "cloudcostguard/internal/config"
    "cloudcostguard/internal/repository/postgres"
    "cloudcostguard/internal/service"
    "go.uber.org/zap"
)

func main() {
    // Initialize logger
    logger, _ := zap.NewProduction()
    defer logger.Sync()

    // Load configuration
    cfg, err := config.Load()
    if err != nil {
        logger.Fatal("Failed to load config", zap.Error(err))
    }

    // Initialize dependencies
    db, err := postgres.NewDB(cfg.Database)
    if err != nil {
        logger.Fatal("Failed to connect to database", zap.Error(err))
    }
    defer db.Close()

    // Initialize services
    pricingRepo := postgres.NewPricingRepository(db)
    pricingCache := cache.NewPricingCache(pricingRepo, logger)
    estimatorSvc := service.NewEstimator(pricingCache, logger)

    // Initialize HTTP server
    router := api.NewRouter(estimatorSvc, logger)
    srv := &http.Server{
        Addr:         cfg.Server.Port,
        Handler:      router,
        ReadTimeout:  15 * time.Second,
        WriteTimeout: 15 * time.Second,
        IdleTimeout:  60 * time.Second,
    }

    // Start server in goroutine
    go func() {
        logger.Info("Starting server", zap.String("port", cfg.Server.Port))
        if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
            logger.Fatal("Server failed", zap.Error(err))
        }
    }()

    // Wait for interrupt signal
    quit := make(chan os.Signal, 1)
    signal.Notify(quit, syscall.SIGINT, syscall.SIGTERM)
    <-quit

    logger.Info("Shutting down server...")

    // Graceful shutdown with timeout
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    if err := srv.Shutdown(ctx); err != nil {
        logger.Fatal("Server forced to shutdown", zap.Error(err))
    }

    logger.Info("Server exited")
}
```

### Fix 2: Thread-Safe Pricing Cache

```go
// internal/cache/pricing_cache.go
package cache

import (
    "context"
    "sync"
    "time"

    "cloudcostguard/backend/pricing"
    "go.uber.org/zap"
)

type PricingCache struct {
    mu       sync.RWMutex
    data     *pricing.PriceList
    repo     PricingRepository
    logger   *zap.Logger
    refreshInterval time.Duration
    stopChan chan struct{}
}

func NewPricingCache(repo PricingRepository, logger *zap.Logger) *PricingCache {
    pc := &PricingCache{
        repo:     repo,
        logger:   logger,
        refreshInterval: 6 * time.Hour,
        stopChan: make(chan struct{}),
    }
    
    // Initial load
    if err := pc.Refresh(context.Background()); err != nil {
        logger.Error("Failed initial cache load", zap.Error(err))
    }
    
    // Start background refresh
    go pc.backgroundRefresh()
    
    return pc
}

func (pc *PricingCache) Get() *pricing.PriceList {
    pc.mu.RLock()
    defer pc.mu.RUnlock()
    return pc.data
}

func (pc *PricingCache) Refresh(ctx context.Context) error {
    priceList, err := pc.repo.LoadPricing(ctx)
    if err != nil {
        return err
    }
    
    pc.mu.Lock()
    pc.data = priceList
    pc.mu.Unlock()
    
    pc.logger.Info("Pricing cache refreshed",
        zap.Int("products", len(priceList.Products)))
    
    return nil
}

func (pc *PricingCache) backgroundRefresh() {
    ticker := time.NewTicker(pc.refreshInterval)
    defer ticker.Stop()
    
    for {
        select {
        case <-ticker.C:
            ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
            if err := pc.Refresh(ctx); err != nil {
                pc.logger.Error("Background refresh failed", zap.Error(err))
            }
            cancel()
        case <-pc.stopChan:
            return
        }
    }
}

func (pc *PricingCache) Stop() {
    close(pc.stopChan)
}
```

### Fix 3: API Middleware Stack

```go
// internal/api/middleware/middleware.go
package middleware

import (
    "net/http"
    "time"

    "github.com/google/uuid"
    "go.uber.org/zap"
)

func RequestLogger(logger *zap.Logger) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            start := time.Now()
            requestID := uuid.New().String()
            
            // Add request ID to context
            ctx := context.WithValue(r.Context(), "request_id", requestID)
            r = r.WithContext(ctx)
            
            // Wrap response writer to capture status
            ww := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
            
            next.ServeHTTP(ww, r)
            
            logger.Info("Request completed",
                zap.String("request_id", requestID),
                zap.String("method", r.Method),
                zap.String("path", r.URL.Path),
                zap.Int("status", ww.statusCode),
                zap.Duration("duration", time.Since(start)),
            )
        })
    }
}

func Recovery(logger *zap.Logger) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            defer func() {
                if err := recover(); err != nil {
                    logger.Error("Panic recovered",
                        zap.Any("error", err),
                        zap.String("path", r.URL.Path),
                    )
                    http.Error(w, "Internal Server Error", http.StatusInternalServerError)
                }
            }()
            next.ServeHTTP(w, r)
        })
    }
}

func RateLimit(requestsPerMinute int) func(http.Handler) http.Handler {
    limiter := rate.NewLimiter(rate.Every(time.Minute/time.Duration(requestsPerMinute)), requestsPerMinute)
    
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            if !limiter.Allow() {
                http.Error(w, "Rate limit exceeded", http.StatusTooManyRequests)
                return
            }
            next.ServeHTTP(w, r)
        })
    }
}
```

### Fix 4: Database Connection Management

```go
// internal/repository/postgres/db.go
package postgres

import (
    "context"
    "database/sql"
    "fmt"
    "time"

    _ "github.com/lib/pq"
)

type Config struct {
    Host            string
    Port            int
    User            string
    Password        string
    Database        string
    SSLMode         string
    MaxOpenConns    int
    MaxIdleConns    int
    ConnMaxLifetime time.Duration
}

func NewDB(cfg Config) (*sql.DB, error) {
    if cfg.User == "" || cfg.Password == "" {
        return nil, fmt.Errorf("database credentials required")
    }
    
    dsn := fmt.Sprintf(
        "host=%s port=%d user=%s password=%s dbname=%s sslmode=%s",
        cfg.Host, cfg.Port, cfg.User, cfg.Password, cfg.Database, cfg.SSLMode,
    )
    
    db, err := sql.Open("postgres", dsn)
    if err != nil {
        return nil, fmt.Errorf("failed to open database: %w", err)
    }
    
    // Configure connection pool
    db.SetMaxOpenConns(cfg.MaxOpenConns)
    db.SetMaxIdleConns(cfg.MaxIdleConns)
    db.SetConnMaxLifetime(cfg.ConnMaxLifetime)
    
    // Verify connection
    ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
    defer cancel()
    
    if err := db.PingContext(ctx); err != nil {
        return nil, fmt.Errorf("failed to ping database: %w", err)
    }
    
    return db, nil
}
```

---

## 4. Testing Strategy

### Current Gaps
- No integration tests for the full API flow
- No load/performance tests
- Mock GitHub server but no full E2E validation
- Missing edge case coverage

### Comprehensive Test Suite

```go
// backend/api/handlers/estimate_test.go
package handlers_test

import (
    "bytes"
    "context"
    "encoding/json"
    "net/http"
    "net/http/httptest"
    "testing"

    "cloudcostguard/backend/estimator"
    "cloudcostguard/internal/api/handlers"
    "github.com/stretchr/testify/assert"
    "github.com/stretchr/testify/require"
)

func TestEstimateHandler_Success(t *testing.T) {
    // Setup
    mockSvc := &mockEstimatorService{
        estimateFunc: func(ctx context.Context, req *estimator.EstimateRequest) (*estimator.EstimationResponse, error) {
            return &estimator.EstimationResponse{
                TotalMonthlyCost: 100.0,
                Currency:         "USD",
                Resources:        []estimator.ResourceCost{},
            }, nil
        },
    }
    
    handler := handlers.NewEstimateHandler(mockSvc, zap.NewNop())
    
    // Create request
    reqBody := `{"plan": {"resource_changes": []}}`
    req := httptest.NewRequest(http.MethodPost, "/estimate", bytes.NewBufferString(reqBody))
    req.Header.Set("Content-Type", "application/json")
    
    // Execute
    w := httptest.NewRecorder()
    handler.ServeHTTP(w, req)
    
    // Assert
    assert.Equal(t, http.StatusOK, w.Code)
    
    var resp estimator.EstimationResponse
    err := json.NewDecoder(w.Body).Decode(&resp)
    require.NoError(t, err)
    assert.Equal(t, 100.0, resp.TotalMonthlyCost)
}

func TestEstimateHandler_InvalidJSON(t *testing.T) {
    mockSvc := &mockEstimatorService{}
    handler := handlers.NewEstimateHandler(mockSvc, zap.NewNop())
    
    req := httptest.NewRequest(http.MethodPost, "/estimate", bytes.NewBufferString("invalid"))
    w := httptest.NewRecorder()
    
    handler.ServeHTTP(w, req)
    
    assert.Equal(t, http.StatusBadRequest, w.Code)
}

func TestEstimateHandler_ServiceError(t *testing.T) {
    mockSvc := &mockEstimatorService{
        estimateFunc: func(ctx context.Context, req *estimator.EstimateRequest) (*estimator.EstimationResponse, error) {
            return nil, fmt.Errorf("service error")
        },
    }
    
    handler := handlers.NewEstimateHandler(mockSvc, zap.NewNop())
    
    reqBody := `{"plan": {"resource_changes": []}}`
    req := httptest.NewRequest(http.MethodPost, "/estimate", bytes.NewBufferString(reqBody))
    w := httptest.NewRecorder()
    
    handler.ServeHTTP(w, req)
    
    assert.Equal(t, http.StatusInternalServerError, w.Code)
}
```

### Load Testing with k6

```javascript
// test/load/estimate_load.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export let options = {
    stages: [
        { duration: '2m', target: 100 },  // Ramp to 100 users
        { duration: '5m', target: 100 },  // Stay at 100 users
        { duration: '2m', target: 0 },    // Ramp down
    ],
    thresholds: {
        http_req_duration: ['p(95)<500'], // 95% of requests under 500ms
        http_req_failed: ['rate<0.01'],   // Less than 1% error rate
    },
};

export default function() {
    const payload = JSON.stringify({
        plan: {
            resource_changes: [
                {
                    address: "aws_instance.test",
                    type: "aws_instance",
                    change: { actions: ["create"] },
                    after: { instance_type: "t2.micro" }
                }
            ]
        }
    });

    const params = {
        headers: { 'Content-Type': 'application/json' },
    };

    const res = http.post('http://localhost:8080/estimate', payload, params);
    
    check(res, {
        'status is 200': (r) => r.status === 200,
        'has total cost': (r) => JSON.parse(r.body).total_monthly_cost !== undefined,
    });

    sleep(1);
}
```

---

## 5. Observability & Monitoring

### Missing Components
- No metrics collection (Prometheus)
- No distributed tracing
- No health/readiness endpoints
- Console logging instead of structured logs

### Implementation

```go
// internal/api/handlers/health.go
package handlers

import (
    "context"
    "encoding/json"
    "net/http"
    "time"
)

type HealthChecker interface {
    CheckHealth(ctx context.Context) error
}

type HealthHandler struct {
    db     HealthChecker
    cache  HealthChecker
}

func (h *HealthHandler) LivenessProbe(w http.ResponseWriter, r *http.Request) {
    // Simple check - is the process alive?
    w.WriteHeader(http.StatusOK)
    json.NewEncoder(w).Encode(map[string]string{"status": "alive"})
}

func (h *HealthHandler) ReadinessProbe(w http.ResponseWriter, r *http.Request) {
    ctx, cancel := context.WithTimeout(r.Context(), 2*time.Second)
    defer cancel()
    
    checks := make(map[string]string)
    allHealthy := true
    
    // Check database
    if err := h.db.CheckHealth(ctx); err != nil {
        checks["database"] = "unhealthy: " + err.Error()
        allHealthy = false
    } else {
        checks["database"] = "healthy"
    }
    
    // Check cache
    if err := h.cache.CheckHealth(ctx); err != nil {
        checks["cache"] = "unhealthy: " + err.Error()
        allHealthy = false
    } else {
        checks["cache"] = "healthy"
    }
    
    status := http.StatusOK
    if !allHealthy {
        status = http.StatusServiceUnavailable
    }
    
    w.WriteHeader(status)
    json.NewEncoder(w).Encode(map[string]interface{}{
        "status": checks,
        "ready":  allHealthy,
    })
}
```

### Prometheus Metrics

```go
// internal/api/middleware/metrics.go
package middleware

import (
    "net/http"
    "strconv"
    "time"

    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    httpRequestsTotal = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "http_requests_total",
            Help: "Total number of HTTP requests",
        },
        []string{"method", "path", "status"},
    )
    
    httpRequestDuration = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "http_request_duration_seconds",
            Help:    "HTTP request latency",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "path"},
    )
    
    estimationDuration = promauto.NewHistogram(
        prometheus.HistogramOpts{
            Name:    "estimation_duration_seconds",
            Help:    "Time to compute cost estimation",
            Buckets: []float64{.01, .05, .1, .5, 1, 2.5, 5},
        },
    )
)

func Metrics() func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            start := time.Now()
            ww := &responseWriter{ResponseWriter: w, statusCode: http.StatusOK}
            
            next.ServeHTTP(ww, r)
            
            duration := time.Since(start).Seconds()
            
            httpRequestsTotal.WithLabelValues(
                r.Method,
                r.URL.Path,
                strconv.Itoa(ww.statusCode),
            ).Inc()
            
            httpRequestDuration.WithLabelValues(
                r.Method,
                r.URL.Path,
            ).Observe(duration)
        })
    }
}
```

---

## 6. Security Hardening

### API Authentication

```go
// internal/api/middleware/auth.go
package middleware

import (
    "crypto/subtle"
    "net/http"
    "strings"
)

func APIKeyAuth(validKeys map[string]bool) func(http.Handler) http.Handler {
    return func(next http.Handler) http.Handler {
        return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
            authHeader := r.Header.Get("Authorization")
            if authHeader == "" {
                http.Error(w, "Missing authorization", http.StatusUnauthorized)
                return
            }
            
            parts := strings.SplitN(authHeader, " ", 2)
            if len(parts) != 2 || parts[0] != "Bearer" {
                http.Error(w, "Invalid authorization format", http.StatusUnauthorized)
                return
            }
            
            apiKey := parts[1]
            
            // Constant-time comparison to prevent timing attacks
            valid := false
            for key := range validKeys {
                if subtle.ConstantTimeCompare([]byte(key), []byte(apiKey)) == 1 {
                    valid = true
                    break
                }
            }
            
            if !valid {
                http.Error(w, "Invalid API key", http.StatusUnauthorized)
                return
            }
            
            next.ServeHTTP(w, r)
        })
    }
}
```

### Input Validation

```go
// internal/api/handlers/validate.go
package handlers

import (
    "fmt"
    "cloudcostguard/backend/terraform"
)

func validatePlan(plan *terraform.Plan) error {
    if plan == nil {
        return fmt.Errorf("plan cannot be nil")
    }
    
    if len(plan.ResourceChanges) > 1000 {
        return fmt.Errorf("too many resources in plan (max 1000)")
    }
    
    for _, rc := range plan.ResourceChanges {
        if rc.Address == "" {
            return fmt.Errorf("resource address cannot be empty")
        }
        if len(rc.Address) > 256 {
            return fmt.Errorf("resource address too long")
        }
    }
    
    return nil
}
```

---

## 7. Database Migrations

```go
// migrations/000001_init_schema.up.sql
CREATE TABLE IF NOT EXISTS aws_prices (
    sku TEXT PRIMARY KEY,
    product_json JSONB NOT NULL,
    terms_json JSONB NOT NULL,
    last_updated TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);

CREATE INDEX idx_aws_prices_last_updated ON aws_prices(last_updated);
CREATE INDEX idx_aws_prices_product_service ON aws_prices USING GIN ((product_json->'attributes'->>'servicecode'));
CREATE INDEX idx_aws_prices_product_location ON aws_prices USING GIN ((product_json->'attributes'->>'location'));

-- Migration tracking
CREATE TABLE IF NOT EXISTS schema_migrations (
    version BIGINT PRIMARY KEY,
    dirty BOOLEAN NOT NULL DEFAULT false,
    applied_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
);
```

```bash
# Use golang-migrate for migrations
migrate -path migrations -database "$DATABASE_URL" up
```

---

## 8. Configuration Management

```go
// internal/config/config.go
package config

import (
    "fmt"
    "time"

    "github.com/kelseyhightower/envconfig"
)

type Config struct {
    Server   ServerConfig
    Database DatabaseConfig
    Cache    CacheConfig
    Logging  LoggingConfig
}

type ServerConfig struct {
    Port            string        `envconfig:"PORT" default:"8080"`
    ReadTimeout     time.Duration `envconfig:"READ_TIMEOUT" default:"15s"`
    WriteTimeout    time.Duration `envconfig:"WRITE_TIMEOUT" default:"15s"`
    ShutdownTimeout time.Duration `envconfig:"SHUTDOWN_TIMEOUT" default:"30s"`
}

type DatabaseConfig struct {
    Host            string        `envconfig:"DB_HOST" required:"true"`
    Port            int           `envconfig:"DB_PORT" default:"5432"`
    User            string        `envconfig:"DB_USER" required:"true"`
    Password        string        `envconfig:"DB_PASSWORD" required:"true"`
    Database        string        `envconfig:"DB_NAME" default:"pricing"`
    SSLMode         string        `envconfig:"DB_SSL_MODE" default:"require"`
    MaxOpenConns    int           `envconfig:"DB_MAX_OPEN_CONNS" default:"25"`
    MaxIdleConns    int           `envconfig:"DB_MAX_IDLE_CONNS" default:"5"`
    ConnMaxLifetime time.Duration `envconfig:"DB_CONN_MAX_LIFETIME" default:"5m"`
}

type CacheConfig struct {
    RefreshInterval time.Duration `envconfig:"CACHE_REFRESH_INTERVAL" default:"6h"`
}

type LoggingConfig struct {
    Level  string `envconfig:"LOG_LEVEL" default:"info"`
    Format string `envconfig:"LOG_FORMAT" default:"json"`
}

func Load() (*Config, error) {
    var cfg Config
    if err := envconfig.Process("CCG", &cfg); err != nil {
        return nil, fmt.Errorf("failed to process env vars: %w", err)
    }
    return &cfg, nil
}
```

---

## 9. Deployment Improvements

### Kubernetes Manifests

```yaml
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloudcostguard-backend
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector:
    matchLabels:
      app: cloudcostguard-backend
  template:
    metadata:
      labels:
        app: cloudcostguard-backend
    spec:
      containers:
      - name: backend
        image: cloudcostguard/backend:latest
        ports:
        - containerPort: 8080
        env:
        - name: DB_HOST
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: host
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-credentials
              key: password
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: cloudcostguard-backend
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 8080
  selector:
    app: cloudcostguard-backend
```

---

## 10. Priority Implementation Roadmap

### Phase 1: Critical Stability (Week 1-2)
- [ ] Implement graceful shutdown
- [ ] Add request timeouts everywhere
- [ ] Proper database connection pooling
- [ ] Thread-safe pricing cache with recovery
- [ ] Basic structured logging

### Phase 2: Observability (Week 3)
- [ ] Health/readiness endpoints
- [ ] Prometheus metrics
- [ ] Request ID tracing
- [ ] Error aggregation

### Phase 3: Security (Week 4)
- [ ] API authentication
- [ ] Rate limiting
- [ ] Input validation
- [ ] Secrets management

### Phase 4: Testing (Week 5-6)
- [ ] Integration test suite
- [ ] Load testing with k6
- [ ] Chaos testing
- [ ] E2E validation

### Phase 5: Production Ops (Week 7-8)
- [ ] Database migrations
- [ ] Kubernetes deployment
- [ ] CI/CD pipeline hardening
- [ ] Runbook documentation

---

## 11. Key Metrics to Track

### SLIs (Service Level Indicators)
- **Availability**: 99.9% uptime
- **Latency**: p95 < 500ms, p99 < 1s
- **Error Rate**: < 0.1%
- **Cache Hit Rate**: > 95%

### Business Metrics
- Estimates per second
- Average plan size
- Regional distribution
- Cost delta distribution

---

## 12. Code Review Checklist

- [ ] Every HTTP client has timeout configured
- [ ] All errors include context (wrap with fmt.Errorf)
- [ ] No blocking operations in handlers
- [ ] Database queries use context with timeout
- [ ] Sensitive data never logged
- [ ] Tests cover happy path + error cases
- [ ] Metrics added for new endpoints
- [ ] Documentation updated

---

## Conclusion

CloudCostGuard has a solid foundation but needs significant production hardening. The priority should be:

1. **Stability first**: Fix graceful shutdown, timeouts, cache resilience
2. **Observability second**: You can't fix what you can't see
3. **Security third**: API auth, rate limiting, validation
4. **Testing last**: Ensure it works under load

**Estimated effort**: 6-8 weeks for a 2-person team to reach production-ready state.
